{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "i17-0368.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APlFUP5RmSWu"
      },
      "source": [
        "Developed By Hassan Murtaza \n",
        "Email: i170368@nu.edu.pk\n",
        "\n",
        "libraries\n",
        "- I use spacy as it has taught in the class\n",
        "- I use regular expression for data cleansing\n",
        "- Algorithms are implemented by myself(not use any library for that\n",
        "- I use pandas for some basic operations\n",
        "- I use confusion metrics for model evaluation\n",
        "\n",
        "First of ALL I build 4 files from given zip and upload these 4 files into local of google colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHIOtoGusIMf"
      },
      "source": [
        "Table of content\n",
        "\n",
        "\n",
        "Note: first 8 steps are with stopwords in text & last 4 steps without stopwords in text\n",
        "1.   Load datasets into local of colab \n",
        "2.   Load datasets into pandas data frame from local csv\n",
        "3.   Data Cleansing\n",
        "4.   Building Documents for model trainings\n",
        "5.   Generating Vocabulary\n",
        "6.   Implementing Helper functions for naive bayes\n",
        "7.   Model Training(NB and Boolean NB)\n",
        "8.   Model Testing\n",
        "9.   Evaluating Models(NB and Bo0lean NB)\n",
        "10.  Stopword Removal\n",
        "11.  Model Building(NB and Boolean NB with stopword removal)\n",
        "12.  Model Testing\n",
        "13.  Model Evaluation\n",
        "14.  Conclusion\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lJmJji6tlAW"
      },
      "source": [
        "1. Load datasets into local of colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "pc2iV1pGJUSO",
        "outputId": "c378bcfa-92fe-4567-c172-b1c8f757cc2e"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "select = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2c6b8e3d-8f4f-487a-9d32-ba7b47b02a99\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2c6b8e3d-8f4f-487a-9d32-ba7b47b02a99\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving stopwords-ur.txt to stopwords-ur.txt\n",
            "Saving Test_Fake.csv to Test_Fake.csv\n",
            "Saving Test_Real.csv to Test_Real.csv\n",
            "Saving Train_Fake.csv to Train_Fake.csv\n",
            "Saving Train_Real.csv to Train_Real.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMIXF6O5uqjw"
      },
      "source": [
        "2. Load datasets into pandas data frame from local csv\n",
        "\n",
        "    Reading csv into Pandas Data Frames\n",
        "\n",
        "    1- Read train (real and fake) csv into pandas, then convert it into single docx\n",
        "\n",
        "    2- Read test (real and fake) csv into pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9Lx5KuPnXAz"
      },
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "unlp = spacy.blank('ur')\n",
        "\n",
        "\n",
        "stopwords = pd.read_csv(\"stopwords-ur.txt\",header=None)\n",
        "trainReal = pd.read_csv(\"Train_Real.csv\")\n",
        "trainFake = pd.read_csv(\"Train_Fake.csv\")\n",
        "# trainRealList = trainReal.drop('labels', inplace=False, axis=1).values.tolist()\n",
        "# trainFakeList = trainFake.drop('labels', inplace=False, axis=1).values.tolist()\n",
        "testReal = pd.read_csv(\"Test_Real.csv\")\n",
        "testFake = pd.read_csv(\"Test_Fake.csv\")\n",
        "testData = pd.concat([testReal, testFake])\n",
        "testData = testData.sample(frac=1)\n",
        "test_X =testData.drop('labels',axis=1,inplace=False).values.tolist() \n",
        "test_Y =testData.drop('features',axis=1,inplace=False).values.tolist()\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfdzEE_ZuwD9"
      },
      "source": [
        "**3. Cleaning Dataset**\n",
        "\n",
        "1- The training data contain Numbers and alphabet, I removed them using regular expression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdGuJibvBoek",
        "outputId": "edc6d6ab-eafe-4116-8d03-5c73b72c7e54"
      },
      "source": [
        "import re\n",
        "def data_cleaning(sent):\n",
        "  punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "  sentCleaned = \"\"\n",
        "  for char in sent:\n",
        "    if char not in punctuations:\n",
        "        sentCleaned = sentCleaned + char\n",
        "  urduHaroof = \"اآب پ ت  ث ٹ ج چ ح خ د ڈ ذ  رڑزژس ش ص ض ط ظ ع غ ف ق ک گ  ل م ن ں و ہ ھ ی ےٿ ڐ ؤئء\"\n",
        "  sentCleaned=re.sub(r'[0-9]', '', sentCleaned)\n",
        "  sentCleaned=re.sub(r'[a-zA-Z]', '', sentCleaned)\n",
        "  return sentCleaned\n",
        "\n",
        "cleanedR = []\n",
        "cleanedF = []\n",
        "cleanedT = []\n",
        "\n",
        "for i in trainReal['features']:\n",
        "  cleanedR.append(data_cleaning(str(i)))\n",
        "\n",
        "for i in trainFake['features']:\n",
        "  cleanedF.append(data_cleaning(str(i)))\n",
        "\n",
        "for i in testData['features']:\n",
        "  cleanedT.append(data_cleaning(str(i)))\n",
        "\n",
        "\n",
        "print(\"Train set and test sets are cleaned successfully\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set and test sets are cleaned successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsMYt3EIvw7a"
      },
      "source": [
        "**4. Generating documents for classes**\n",
        "\n",
        "It means, divide the dataset into real examples and fake examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzbmCH7lv0KG",
        "outputId": "79d455d6-b4ad-4d9d-e94d-6de110710ab0"
      },
      "source": [
        "docxReal = (\" \".join(map(str,cleanedR))).split()\n",
        "docxFake = (\" \".join(map(str,cleanedF))).split()\n",
        "\n",
        "print(\"Size of real doc\", len(docxReal))\n",
        "print(\"Size of Fake doc\", len(docxFake))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of real doc 123774\n",
            "Size of Fake doc 64543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PMsXqo3u0K4"
      },
      "source": [
        "**5. Generating Vocabulary**\n",
        "\n",
        "This is the most important part where I build vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWQXn3D8Cc0e",
        "outputId": "440e572e-7aed-4f83-d5ca-7411d2aa731e"
      },
      "source": [
        "v1 = (\" \".join(map(str,cleanedR))).split()\n",
        "v2 = (\" \".join(map(str,cleanedF))).split()\n",
        "vocab1 = list(set(v1))\n",
        "vocab2 = list(set(v2))\n",
        "# It cancatenate 2 vcabs into single and then remove the redundant words\n",
        "vocabulary = list(set(vocab1 + vocab2))\n",
        "\n",
        "print(\"Vocab Length = \", len(vocabulary))\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Length =  14256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzoBVYYp70l0"
      },
      "source": [
        "**6. Helper Functions**\n",
        "\n",
        "This section contains helper functions which are necessary for Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWUVJq-8JFX0",
        "outputId": "a6c39e2c-c78c-484c-a048-e04af2c7990e"
      },
      "source": [
        "# It returns the count of a word in particular docx\n",
        "def count_word(word,docx):\n",
        "  return len(re.findall(word, str(docx)))\n",
        "\n",
        "print((count_word('انہیں', docxReal)+1) / (len(docxReal) + len(vocabulary)))\n",
        "# It returns the prior class probability\n",
        "def find_prior(nb):\n",
        "    if nb == 'real':\n",
        "      prob = len(cleanedR) / (len(cleanedF) + len(cleanedR))\n",
        "      return prob \n",
        "    elif nb =='fake':\n",
        "      prob = len(cleanedF) / (len(cleanedF) + len(cleanedR))\n",
        "      return prob\n",
        "\n",
        "\n",
        "\n",
        "# This Funtion calculate the conditional probability\n",
        "def conditional_probability(docx, vocabulary):\n",
        "  probList = []\n",
        "  totalsize = len(docx) + len(vocabulary)\n",
        "  print(totalsize)\n",
        "  for i in vocabulary:\n",
        "    probList.append([i , (count_word(i, docx)+1) / (len(docxReal) + len(vocabulary))])\n",
        "  return probList\n",
        "\n",
        "\n",
        "# This is a helper function for test method\n",
        "def probability(word, model):\n",
        "  prob = 0.0003912193001521408\n",
        "  for i in model:\n",
        "    if i[0] == word:\n",
        "      prob = i[1] \n",
        "  return prob\n",
        "\n",
        "\n",
        "# This is a train function for traing the data\n",
        "def train(classs, vocab):\n",
        "  return conditional_probability(classs, vocab)\n",
        "\n",
        "# This is test method for test the models on test data\n",
        "import numpy as np\n",
        "def test(Real, Fake, testData):\n",
        "  import random\n",
        "  result = list()\n",
        "  for sent in testData:\n",
        "    pR = []\n",
        "    pF = []\n",
        "    for word in sent:\n",
        "      #print(\" Real \", word,probability(word, Real))\n",
        "      #print(\" Fake \", word,probability(word, Fake))\n",
        "      \n",
        "       pR.append(probability(word, Real))\n",
        "       pF.append(probability(word, Real))\n",
        "    priorR = np.log(find_prior(\"real\"))\n",
        "    priorF = np.log(find_prior(\"fake\"))\n",
        "\n",
        "    pr = np.sum(np.log(pR)) + priorR\n",
        "    pf = np.sum(np.log(pF)) + priorF\n",
        "\n",
        "\n",
        "    if np.argmax([pr,pf]) == 0:\n",
        "      result.append('Real')\n",
        "    elif np.argmaz([pr,pf]) == 1:\n",
        "       result.append(\"Fake\")\n",
        "  return result\n",
        "\n",
        "# It removes the duplicates words\n",
        "def unique_list(l):\n",
        "    ulist = []\n",
        "    [ulist.append(x) for x in l if x not in ulist]\n",
        "    return ulist\n",
        "\n",
        "\n",
        "# It will be used for removing stopwords\n",
        "def remove_stopwords(word):\n",
        "  for i in stopwords[stopwords.columns[0]]:\n",
        "    if word == i:\n",
        "      return True\n",
        "  return False"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0008983554299789901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRBFXvX5jyuI"
      },
      "source": [
        "7. Naive Bayes Implemention (without removing stopwords)\n",
        "\n",
        "- Here, first I train model on both classes\n",
        "- I also use laplace smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_YMoqT0Kmwz",
        "outputId": "0e390df7-d5bf-4dd7-862e-55ec18cf41cf"
      },
      "source": [
        "realClassModel = train(docxReal, vocabulary)\n",
        "fakeClassModel = train(docxFake, vocabulary)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "138030\n",
            "78799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcXuR6cxrQSM"
      },
      "source": [
        "Boolean Naive Bayes Implemention (without removing stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VPQNHA1rPka",
        "outputId": "bd7d656a-0e96-4569-91f9-5076380a7c8d"
      },
      "source": [
        "# First of all removing redundant words from each review \n",
        "trainRSetBoolean = list()\n",
        "trainFSetBoolean = list()\n",
        "testSetBoolean = list()\n",
        "\n",
        "for i in cleanedR:\n",
        "  trainRSetBoolean.append(' '.join(unique_list(str(i).split())))\n",
        "\n",
        "for i in cleanedF:\n",
        "  trainFSetBoolean.append(' '.join(unique_list(str(i).split())))\n",
        "\n",
        "for i in cleanedT:\n",
        "  testSetBoolean.append(' '.join(unique_list(str(i).split())))\n",
        "\n",
        "# Now generating documents\n",
        "docxBReal = (\" \".join(map(str,trainRSetBoolean))).split()\n",
        "docxBFake = (\" \".join(map(str,trainFSetBoolean))).split()\n",
        "\n",
        "\n",
        "print(\"Total words contain docxBReal==\", len(docxBReal))\n",
        "print(\"Total words contain docxBFake==\", len(docxBFake))\n",
        "# Boolean Naive Bayes Training\n",
        "boolRModel = train(docxBReal, vocabulary)\n",
        "boolFModel = train(docxBFake, vocabulary)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words contain docxBReal== 59994\n",
            "Total words contain docxBFake== 33634\n",
            "74250\n",
            "47890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6umdh0JZcEQj"
      },
      "source": [
        "**8.Model Testing**\n",
        "\n",
        "Here I am Testing my model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXGUutzEaHFg"
      },
      "source": [
        "testNB = list()\n",
        "testBNB = list()\n",
        "for i in cleanedT:\n",
        "  testNB.append(str(i).split())\n",
        "for i in testSetBoolean:\n",
        "  testBNB.append(str(i).split())\n",
        "\n",
        "predNB = test(Real=realClassModel, Fake=fakeClassModel, testData=testNB)\n",
        "predBNB = test(Real=boolRModel, Fake=boolFModel, testData=testBNB)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be6fJOvxmPhw"
      },
      "source": [
        "**9. Model Evaluation**\n",
        "\n",
        "Confusion Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBcwCrP1BlB6"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPcPO_VZcrOi",
        "outputId": "4d7afeaf-cd36-4e87-e5f9-bbfddde5326e"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(\"Simple Naive Bayes Statistics Model Evaluation\")\n",
        "print(metrics.confusion_matrix(test_Y, predNB, labels=[\"Real\",\"Fake\"]))\n",
        "print(metrics.classification_report(test_Y, predNB, labels=[\"Real\",\"Fake\"]))\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "print(\"Boolean Naive Bayes Statistics Model Evaluation\")\n",
        "print(metrics.confusion_matrix(test_Y, predBNB, labels=[\"Real\",\"Fake\"]))\n",
        "print(metrics.classification_report(test_Y, predBNB, labels=[\"Real\",\"Fake\"]))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simple Naive Bayes Statistics Model Evaluation\n",
            "[[150   0]\n",
            " [ 90   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.62      1.00      0.77       150\n",
            "        Fake       0.00      0.00      0.00        90\n",
            "\n",
            "    accuracy                           0.62       240\n",
            "   macro avg       0.31      0.50      0.38       240\n",
            "weighted avg       0.39      0.62      0.48       240\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Boolean Naive Bayes Statistics Model Evaluation\n",
            "[[150   0]\n",
            " [ 90   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.62      1.00      0.77       150\n",
            "        Fake       0.00      0.00      0.00        90\n",
            "\n",
            "    accuracy                           0.62       240\n",
            "   macro avg       0.31      0.50      0.38       240\n",
            "weighted avg       0.39      0.62      0.48       240\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PPNN6_UWyCb"
      },
      "source": [
        "10. Stopword Removal\n",
        "\n",
        "    - Preparing Train Set for NB and Boolean NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NSBAA0gRIey",
        "outputId": "a961ffd7-8676-4776-abdf-5ff4096e1943"
      },
      "source": [
        "docxSWRealNB = []\n",
        "docxSWFakeNB = []\n",
        "docxSWRealBNB = []\n",
        "docxSWFakeBNB = []\n",
        "testSet = []\n",
        "\n",
        "\n",
        "for i in docxReal:\n",
        "  if remove_stopwords(i):\n",
        "    continue\n",
        "  docxSWRealNB.append(i)\n",
        "print(\"Length of docx with real reviews for Niave Bayes\", len(docxSWRealNB))\n",
        "\n",
        "for i in docxFake:\n",
        "  if remove_stopwords(i):\n",
        "    continue\n",
        "  docxSWFakeNB.append(i)\n",
        "print(\"Length of docx with Fake reviews for Niave Bayes\", len(docxSWFakeNB))\n",
        "\n",
        "\n",
        "for i in docxBReal:\n",
        "  if remove_stopwords(i):\n",
        "    continue\n",
        "  docxSWRealBNB.append(i)\n",
        "print(\"Length of docx with real reviews for Boolean Niave Bayes\", len(docxSWRealBNB))\n",
        "\n",
        "for i in docxBFake:\n",
        "  if remove_stopwords(i):\n",
        "    continue\n",
        "  docxSWFakeBNB.append(i)\n",
        "print(\"Length of docx with Fake reviews for Boolean Niave Bayes\", len(docxSWFakeBNB))\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of docx with real reviews for Niave Bayes 96871\n",
            "Length of docx with Fake reviews for Niave Bayes 49989\n",
            "Length of docx with real reviews for Boolean Niave Bayes 52464\n",
            "Length of docx with Fake reviews for Boolean Niave Bayes 29162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMd0xtLigPeZ"
      },
      "source": [
        "\n",
        "  - Preparing Test set for NB and Boolean NB "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH5fvsJIXrEG"
      },
      "source": [
        "testSWNB = []\n",
        "testSWBNB = []\n",
        "\n",
        "for i in cleanedT:\n",
        "  sent = i.split()\n",
        "  row = []\n",
        "  for j in sent:\n",
        "    if remove_stopwords(j):\n",
        "      continue\n",
        "    row.append(j)\n",
        "  testSWNB.append(row)\n",
        "\n",
        "\n",
        "for i in testSetBoolean:\n",
        "  \n",
        "  sent = i.split()\n",
        "  row = []\n",
        "  for j in sent:\n",
        "    if remove_stopwords(j):\n",
        "      continue\n",
        "    row.append(j)\n",
        "  testSWBNB.append(row)\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkkCwTUlhmlz"
      },
      "source": [
        "11. Model Training (NB and Boolean NB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6facT-2hdKA",
        "outputId": "06a8b32b-9324-4427-89cb-5d303fa51cfa"
      },
      "source": [
        "nbSWRModel = train(docxSWRealNB, vocabulary)\n",
        "nbSWFModel = train(docxSWFakeNB, vocabulary)\n",
        "\n",
        "bnbSWRModel = train(docxSWRealBNB, vocabulary)\n",
        "bnbSWFModel = train(docxSWFakeBNB, vocabulary)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "111127\n",
            "64245\n",
            "66720\n",
            "43418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWOPK_0VicNs"
      },
      "source": [
        "**12. Model Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnrEVEEjiatC"
      },
      "source": [
        "predSWNB = test(Real=nbSWRModel, Fake=nbSWFModel, testData=testSWNB)\n",
        "predSWBNB = test(Real=bnbSWRModel, Fake=bnbSWFModel, testData=testSWBNB)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51MG4yaZjBLq"
      },
      "source": [
        "13. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1Tx6seHjFJ7",
        "outputId": "bec9168c-6ec5-49e8-ef04-5bdf50896f83"
      },
      "source": [
        "print(\"Removed Stopwords: Simple Naive Bayes Statistical Model Evaluation\")\n",
        "print(metrics.confusion_matrix(test_Y, predSWNB, labels=[\"Real\",\"Fake\"]))\n",
        "print(metrics.classification_report(test_Y, predSWNB, labels=[\"Real\",\"Fake\"]))\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "print(\"Removed Stopwords: Boolean Naive Bayes Statistical Model Evaluation\")\n",
        "print(metrics.confusion_matrix(test_Y, predSWBNB, labels=[\"Real\",\"Fake\"]))\n",
        "print(metrics.classification_report(test_Y, predSWBNB, labels=[\"Real\",\"Fake\"]))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Removed Stopwords: Simple Naive Bayes Statistical Model Evaluation\n",
            "[[150   0]\n",
            " [ 90   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.62      1.00      0.77       150\n",
            "        Fake       0.00      0.00      0.00        90\n",
            "\n",
            "    accuracy                           0.62       240\n",
            "   macro avg       0.31      0.50      0.38       240\n",
            "weighted avg       0.39      0.62      0.48       240\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Removed Stopwords: Boolean Naive Bayes Statistical Model Evaluation\n",
            "[[150   0]\n",
            " [ 90   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.62      1.00      0.77       150\n",
            "        Fake       0.00      0.00      0.00        90\n",
            "\n",
            "    accuracy                           0.62       240\n",
            "   macro avg       0.31      0.50      0.38       240\n",
            "weighted avg       0.39      0.62      0.48       240\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJjtT9jsklFw"
      },
      "source": [
        "Synopsis\n",
        "\n",
        "After a very long and hectic implementation of 4 type of model, I come to conclusion which is stated below\n",
        "  \n",
        "\n",
        "1.   Boolean Naive bayes and simple Naive Bayes  perform similar in term of evaluations. They Accuracy Score by Both these Algorithms has 62 percent.\n",
        "\n",
        "2.   Removing stopwords cause no betterment in the acuuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffN4x6KKGjkQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}